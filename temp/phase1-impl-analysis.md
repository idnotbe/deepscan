# Phase 1: Implementation Analysis - All 17 Python Modules

> Generated by impl-scanner for gap analysis comparison.
> Source: `.claude/skills/deepscan/scripts/`

---

## 1. `__init__.py`

**Module purpose**: Package initialization and version declaration.

**Constants**:
- `__version__ = "2.0.0"` -- Plugin version string

**External dependencies**: None

**Notes**: Docstring mentions "Phase 7: Full feature set with parallel processing, adaptive chunking, and model escalation."

---

## 2. `aggregator.py`

**Module purpose**: Result aggregation for the REDUCE phase. Merges findings from multiple chunks, handles deduplication, contradiction detection, and confidence scoring. Also contains FINAL marker parsing (FR-007).

### Classes

#### `ResultAggregator`
- **`__init__(self, similarity_threshold: float = 0.7)`** -- Initialize aggregator. Default threshold 0.7 (REQ_02 FR-006).
- **`aggregate_findings(self, chunk_results, original_query, deleted_files=None) -> dict`** -- Main aggregation method.
  - Returns dict with keys: `aggregated_findings`, `total_findings`, `unique_findings`, `deduplication_ratio`, `contradictions`, `needs_manual_review`, `filtered_deleted_files`, `verification_required_count`, `verification_required_findings`
  - P7-003: Ghost Findings Cleanup -- filters findings from deleted files
  - P3.3-FIX: NEEDS_VERIFICATION prefix parsing
- **`format_summary(self, aggregated, max_findings=10) -> str`** -- Format results as human-readable summary.
- **`_normalize_deleted_paths(self, deleted_files) -> set[str]`** -- Cross-platform path normalization (backslash -> forward slash, lowercase).
- **`_normalize_path(self, path) -> str`** -- Single path normalization.
- **`_is_ghost_finding(self, finding, chunk_id, deleted_paths) -> bool`** -- Check if finding references a deleted file (checks chunk_id, finding.location.file, finding.evidence).
- **`_group_by_similarity(self, findings) -> list[list[dict]]`** -- Token-based blocking optimization for grouping similar findings. Near-linear complexity for diverse text.
- **`_text_similarity(self, a, b) -> float`** -- SequenceMatcher-based similarity (0.0 to 1.0).
- **`_can_be_similar(self, a, b) -> bool`** -- Quick filter: length ratio check (0.5 minimum) and token overlap.
- **`_build_token_index(self, findings) -> dict[str, list[int]]`** -- Inverted index by first 5 words (min 3 chars).
- **`_confidence_score(self, confidence) -> int`** -- Converts "high"->3, "medium"->2, "low"->1 (case-insensitive, unknown defaults to 1).
- **`_relevance_score(self, finding, query) -> float`** -- Keyword overlap scoring.
- **`_detect_contradictions(self, merged) -> list[dict]`** -- Negation-based heuristic with 0.4 similarity threshold. Checks bidirectional negation words: "no ", "not ", "never ", "without ", "n't ".

#### `FinalMarkerType` (Enum)
- `FINAL` -- Direct answer: `FINAL(json_content)`
- `FINAL_VAR` -- Variable reference: `FINAL_VAR(variable_name)`
- `NEEDS_MORE` -- Need more processing: `NEEDS_MORE(reason)`
- `UNABLE` -- Cannot complete: `UNABLE(reason)`

#### `ParsedFinalMarker` (dataclass)
- Fields: `marker_type`, `content`, `raw_match`

### Functions

- **`aggregate_chunk_results(chunk_results, query, similarity_threshold=0.7) -> dict`** -- Convenience wrapper.
- **`parse_final_markers(text) -> list[ParsedFinalMarker]`** -- Parse FINAL/FINAL_VAR/NEEDS_MORE/UNABLE markers from text.
- **`extract_final_answer(text, namespace=None) -> tuple[str, Any] | None`** -- Extract final answer using markers. Returns tuple like `("final", content)` or `("final_var_error", message)`.
- **`has_final_marker(text) -> bool`** -- Quick check for any final marker presence.

### External dependencies
- `collections.defaultdict`, `difflib.SequenceMatcher`, `json`, `re`, `dataclasses`, `enum`

---

## 3. `ast_chunker.py`

**Module purpose**: AST-based semantic chunking using tree-sitter. Splits code at semantic boundaries (class, function, method). Includes Coalescing Iterator that captures ALL content including gaps.

### Constants/Config

- **`SCOPE_TYPES_BY_LANGUAGE`** -- Scope boundary node types per language (python, javascript, typescript, java, go). Python includes `decorated_definition`.
- **`COMPOUND_TYPES_BY_LANGUAGE`** -- Compound statement types per language (if, for, while, try, etc.).
- **`LANGUAGE_BY_EXTENSION`** -- File extension to language mapping. Supports: `.py`, `.pyw`, `.js`, `.jsx`, `.mjs`, `.ts`, `.tsx`, `.java`, `.go`.
- **`GC_EVERY_N_FILES = 50`** -- Garbage collection interval.
- **`MEMORY_THRESHOLD_MB = 500`** -- Memory pressure threshold.
- **`MIN_FILES_FOR_MEMORY_GC = 10`** -- Minimum files before memory-pressure GC.
- **`TOKEN_SAFETY_MARGIN = 0.80`** -- 80% token utilization per chunk.

### Classes

#### `SemanticChunk` (Pydantic BaseModel)
- Fields: `chunk_id` (default "pending"), `content`, `start_line`, `end_line`, `node_type`, `language`, `file_path`, `char_count`, `token_count`, `is_fallback`
- **`model_post_init`** -- Auto-calculates `char_count` and `token_count`.
- **`with_deterministic_id` (classmethod)** -- Factory with deterministic chunk ID.

### Functions

- **`detect_language(file_path) -> str | None`** -- Language detection from file extension.
- **`count_tokens(text) -> int`** -- Token estimation (~4 chars/token, adjusted for whitespace).
- **`generate_chunk_id(file_path, start_line, content) -> str`** -- 8-char hex SHA-256 hash. Uses incremental hashing.
- **`get_line_number(content, byte_offset) -> int`** -- Deprecated O(N) line number lookup.
- **`split_text_lines(text, max_chars) -> list[str]`** -- Line-aware text splitting.
- **`get_parser_safe(language) -> Any | None`** -- Safe tree-sitter parser import. Tries `tree-sitter-language-pack` first, then `tree-sitter-languages` as fallback.
- **`chunk_file_ast(file_path, max_chunk_chars=150_000, max_chunk_tokens=40_000, max_depth=50, project_root=None) -> list[SemanticChunk]`** -- Main entry point. Security: enforces file within project_root via `resolve(strict=True)` + `relative_to()`.
- **`extract_scopes_v2(node, content, chunks, max_chars, max_tokens, language, file_path, depth=0, max_depth=50, last_byte=0) -> int`** -- Coalescing Iterator. Handles gaps, ERROR nodes, depth limits.
- **`fallback_text_chunk(file_path, max_chunk_chars, project_root=None, overlap_lines_count=5) -> list[SemanticChunk]`** -- Text-based fallback with line overlap.
- **`chunk_files_safely(file_paths, max_chunk_chars=150_000, gc_interval=50, memory_threshold_mb=500, project_root=None) -> Iterator[SemanticChunk]`** -- Generator with memory management. Uses psutil if available.
- **`chunk_files_to_list(...) -> list[SemanticChunk]`** -- Convenience wrapper returning list.

### Security
- Path traversal protection via `resolve(strict=True)` + `relative_to()` (lines 400-420)
- Max recursion depth enforcement (DoS protection)

### External dependencies
- `gc`, `hashlib`, `logging`, `pathlib`, `pydantic`, `tree_sitter_language_pack` or `tree_sitter_languages` (optional)

---

## 4. `cancellation.py`

**Module purpose**: Work cancellation with "Double Tap" pattern. First Ctrl+C: graceful shutdown + checkpoint. Second Ctrl+C: force quit.

### Constants
- `EXIT_CODE_FORCE_QUIT = 130` -- Unix convention (128 + SIGINT)

### Classes

#### `CancellationError` (Exception)
- Default message: "Operation cancelled by user"

#### `CancellationManager`
- **`__init__(self, graceful_timeout=10.0, on_graceful=None, on_force=None, on_cleanup=None)`** -- Validates graceful_timeout > 0.
- **`setup(self)`** -- Installs SIGINT handler (all platforms) and SIGTERM (Unix only).
- **`_handle_signal(self, signum, frame)`** -- Signal handler. Uses `sys.stderr.write()` (re-entrancy safe). Executes callbacks in separate threads to avoid deadlock.
- **`_execute_graceful_callbacks(self)`** -- Runs cleanup then graceful callbacks in separate thread.
- **`_safe_callback(self, callback, name)`** -- Exception-safe callback execution.
- **`_graceful_timeout_thread(self)`** -- Uses `os._exit()` (not `sys.exit()`) for reliable process termination.
- **`_force_exit(self)`** -- Flushes stdout/stderr before `os._exit(130)`.
- **`is_cancelled(self) -> bool`** -- Thread-safe check.
- **`is_force_quit(self) -> bool`** -- Thread-safe check.
- **`check_and_raise(self)`** -- Raises `CancellationError` if force quit.
- **`mark_completed(self)`** -- Prevents timeout thread from force quitting after graceful save.
- **`reset(self)`** -- For testing only.
- **`show_resume_instructions(session_hash)` (static)** -- Prints resume command with ANSI colors.

### Functions

- **`get_cancellation_manager(graceful_timeout=10.0, ..., reset=False) -> CancellationManager`** -- Thread-safe singleton factory.
- **`atomic_write_with_cancellation(file_path, content, cancel_mgr=None, max_retries=3, retry_delay=0.1) -> bool`** -- Atomic write (temp-file-and-rename). Only aborts on Force Quit (not Graceful). Windows retry loop.

### External dependencies
- `logging`, `os`, `signal`, `sys`, `threading`, `time`

---

## 5. `checkpoint.py`

**Module purpose**: Checkpoint management for session recovery. Saves/loads session state for resume after interruption.

### Constants (imported from models)
- `MAX_CHECKPOINT_READ_SIZE`, `MAX_CHECKPOINT_WRITE_SIZE`, `SESSION_HASH_PATTERN`

### Classes

#### `CheckpointTooLargeError` (Exception)
- Raised when checkpoint exceeds read limit.

#### `Checkpoint` (Pydantic BaseModel)
- Fields: `checkpoint_id`, `session_id`, `phase`, `batch_index`, `completed_chunks`, `pending_chunks`, `partial_results`, `created_at`

#### `CheckpointManager`
- **`__init__(self, session_hash, cache_root=None)`** -- Validates session_hash with regex AND `..` check AND post-resolution path traversal check. Default cache root: `~/.claude/cache/deepscan`.
- **`save_checkpoint(self, state, batch_index, cancel_mgr=None, max_retries=3, retry_delay=0.1) -> Checkpoint`** -- Atomic write with Windows retry. Logs checkpoint size. Warns if exceeds write limit.
- **`load_checkpoint(self) -> Checkpoint | None`** -- Size check before reading (DoS protection). Asymmetric limits: READ=100MB, WRITE=20MB.
- **`clear_checkpoint(self) -> bool`** -- Delete checkpoint file.
- **`has_checkpoint(self) -> bool`** -- Existence check.
- **`get_checkpoint_info(self) -> dict | None`** -- Summary without full load.

### Functions

- **`restore_state_from_checkpoint(state, checkpoint)`** -- Modifies state in-place: marks completed chunks, restores phase, results, progress.

### External dependencies
- `json`, `logging`, `os`, `time`, `datetime`, `pathlib`, `pydantic`

---

## 6. `constants.py`

**Module purpose**: Bottom-layer constants module. No imports from other deepscan modules.

### Timeout & Size Limits
| Constant | Value | Description |
|----------|-------|-------------|
| `DEFAULT_EXEC_TIMEOUT` | 5 | REPL execution timeout (seconds) |
| `MIN_CHUNKING_TIMEOUT` | 30 | Minimum chunking timeout |
| `MAX_CHUNKING_TIMEOUT` | 120 | Hard cap on chunking timeout |
| `TIMEOUT_PER_MB` | 2 | Seconds per MB for dynamic timeout |
| `GREP_TIMEOUT` | 10 | ReDoS protection timeout |
| `MAX_OUTPUT_SIZE` | 500,000 | 500KB max output per operation |
| `MAX_CLI_OUTPUT` | 100,000 | 100KB max for CLI display |
| `MAX_CONTEXT_PREVIEW` | 50,000 | 50KB max for peek operations |
| `MAX_GREP_CONTENT_SIZE` | 5,000,000 | 5MB limit per grep |

### Helper Names
- `HELPER_NAMES: frozenset[str]` -- 20 helpers: `peek`, `peek_head`, `peek_tail`, `grep`, `grep_file`, `chunk_indices`, `write_chunks`, `add_buffer`, `get_buffers`, `clear_buffers`, `add_result`, `add_results_from_file`, `set_phase`, `set_final_answer`, `get_status`, `context_length`, `is_lazy_mode`, `get_tree_view`, `preview_dir`, `load_file`

### SAFE_BUILTINS
- 35 entries: `len`, `str`, `int`, `float`, `bool`, `list`, `dict`, `tuple`, `set`, `print`, `range`, `enumerate`, `zip`, `map`, `filter`, `min`, `max`, `sum`, `sorted`, `reversed`, `abs`, `round`, `isinstance`, `type`, `repr`, `True`, `False`, `None`, `all`, `any`, `slice`, `dir`, `vars`, `hasattr`, `callable`, `id`
- **Notably absent** (blocked): `exec`, `eval`, `open`, `__import__`, `getattr`, `setattr`, `delattr`, `compile`, `globals`, `locals`, `input`, `breakpoint`
- **Introspection included**: `type`, `vars`, `dir`, `hasattr` (with note about adversarial testing)

### REDOS_PATTERNS
- 12 patterns detecting nested quantifiers, non-capturing group variants, named group variants, alternation with overlap, character class in nested quantifier, unbounded repetition patterns.

### Chunk Size Configuration
| Extension | Chunk Size | Category |
|-----------|-----------|----------|
| `.py`, `.java`, `.ts`, `.tsx`, `.js`, `.jsx`, `.go`, `.rs`, `.c`, `.cpp`, `.h`, `.hpp`, `.cs`, `.rb`, `.php`, `.swift`, `.kt` | 100,000 | Code |
| `.json`, `.yaml`, `.yml`, `.toml` | 80,000 | Config |
| `.xml`, `.sql` | 100,000 | Code |
| `.md`, `.rst` | 200,000 | Docs |
| `.txt` | 250,000 | Docs |
| `.html` | 150,000 | Docs |
| Default | 150,000 | -- |

### Other Constants
| Constant | Value | Description |
|----------|-------|-------------|
| `DEFAULT_PROGRESS_MAX_SIZE` | 10MB | Progress file rotation limit |
| `WATCH_POLL_INTERVAL` | 2 | Seconds between progress.jsonl polls |
| `DEFAULT_LAZY_DEPTH` | 3 | Max lazy mode directory depth |
| `DEFAULT_LAZY_FILE_LIMIT` | 50 | Max files in lazy mode |
| `DEFAULT_TREE_VIEW_LIMIT` | 10,000 | Safety cap for tree view entries |
| `MAX_RECOMMENDED_CHUNKS` | 100 | Warning threshold for chunk count |
| `MAX_ABSOLUTE_CHUNKS` | 500 | Hard limit for chunk count |

### Utility Functions
- **`calculate_chunking_timeout(context_size_bytes) -> int`** -- Dynamic timeout: `max(30, min(size_mb * 2, 120))`.
- **`truncate_output(text, max_size=500_000, suffix=...) -> str`** -- Truncation with informative suffix.
- **`detect_content_type(path, file_extensions) -> tuple[str, int]`** -- Returns `(content_type_description, chunk_size)`. Categories: `code:`, `config:`, `docs:`, `other:`.

### External dependencies
- `collections.Counter`, `collections.abc.Iterable`, `pathlib.Path`

---

## 7. `deepscan_engine.py`

**Module purpose**: Main CLI entry point and orchestrator for all DeepScan commands. Phase 7 engine with parallel sub-agent processing.

### CLI Commands

| Command | Function | Description |
|---------|----------|-------------|
| `init <path>` | `cmd_init` | Initialize session with context path |
| `status` | `cmd_status` | Show current session status |
| `exec -c <code>` | `cmd_exec` | Execute sandboxed Python code |
| `reset` | `cmd_reset` | Reset current session |
| `export-results <path>` | `cmd_export_results` | Export results to JSON file |
| `list` | `cmd_list` | List all sessions |
| `resume [hash]` | `cmd_resume` | Resume a session (default: most recent) |
| `abort <hash>` | `cmd_abort` | Delete a session permanently |
| `clean [--older-than N]` | `cmd_clean` | Clean old sessions (default: 7 days) |
| `map` | `cmd_map` | Run MAP phase (parallel chunk processing) |
| `progress [--watch]` | `cmd_progress` | Show or watch progress |
| `reduce` | `cmd_reduce` | Run REDUCE phase (aggregate findings) |

### CLI Init Arguments
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `context_path` | positional | required | Path to analyze |
| `-q/--query` | str | None | Initial query |
| `-a/--adaptive` | flag | False | Adaptive chunk sizing |
| `--incremental` | flag | False | Only process changed files |
| `--previous-session` | str | None | Session hash for delta |
| `--lazy` | flag | False | Structure only, no content |
| `--target` | str (repeatable) | None | Target specific paths |
| `--depth` | int | 3 | Max lazy traversal depth |
| `--agent-type` | choice | "general" | general/security/architecture/performance |
| `--force` | flag | False | Overwrite existing session |

### CLI Map Arguments
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `-i/--instructions` | flag | False | Show Task tool instructions |
| `-e/--escalate` | flag | False | Retry failed chunks with sonnet |
| `-o/--output` | str | None | Write instructions to file |
| `--batch` | int | None | Show specific batch (1-indexed) |
| `--limit` | int | 5 | Max chunks in instructions |

### CLI Exec Arguments
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `-c/--code` | str | required | Code to execute |
| `-t/--timeout` | int | None | Execution timeout (auto-detected) |

### CLI Shortcuts (FR-018.5)
| Shortcut | Expansion |
|----------|-----------|
| `?` | `status` |
| `! "code"` | `exec -c "code"` |
| `+` | `resume` |
| `+ hash` | `resume hash` |
| `x hash` | `abort hash` |
| `<path>` | `init <path>` (if path exists) |

### Security (cmd_exec sandbox)
- **Layer 1: Forbidden Patterns** (lines 345-361): `__import__`, `exec(`, `eval(`, `compile(`, `open(`, `os.`, `subprocess`, `sys.`, `__globals__`, `__class__`, `__bases__`, `__closure__`, `getattr(`, `setattr(`, `delattr(`
- **Layer 2: AST Whitelist** (lines 382-441): Explicit allow list of ~50 AST node types. Blocks: `FunctionDef`, `ClassDef`, `AsyncFunctionDef`, `Import`, `ImportFrom`, `Global`, `Nonlocal`, `Yield`, `YieldFrom`, `Await`, `Try`, `Raise`, `With`, `Assert`, `Match`. Allows: comprehensions, lambda, f-strings, keyword args.
- **Layer 3: Dangerous Attribute Blocking** (lines 453-462): Blocks all `_`-prefixed attributes and 20 specific dunder attributes (`__class__`, `__bases__`, `__subclasses__`, `__mro__`, `__globals__`, `__code__`, `__closure__`, `__func__`, `__self__`, `__dict__`, `__doc__`, `__module__`, `__builtins__`, `__import__`, `__loader__`, `__spec__`, `__annotations__`, `__wrapped__`, `__qualname__`).
- **MAX_CODE_LENGTH = 100,000** -- Input length limit for ReDoS prevention.

### Key Functions

- **`process_map_phase(manager, batch_size=None, sequential_fallback=True, cancel_mgr=None) -> dict`** -- Processes chunks in batches. Supports cancellation. Tracks placeholders separately. Falls back to sequential after 2 consecutive >50% failure batches.
- **`generate_map_instructions(manager, model="haiku", failed_chunks_only=False, limit=5, batch_num=None) -> str`** -- Generates Task tool instructions for Claude Code.
- **`_process_batch_parallel(...) -> list[dict]`** -- CLI mode: generates placeholder results.
- **`_process_batch_sequential(...) -> list[dict]`** -- Sequential fallback mode.
- **`_expand_cli_shortcuts(argv) -> list[str]`** -- CLI shortcut expansion.
- **`main() -> int`** -- Entry point.

### Windows UTF-8 Fix
- Reconfigures stdout/stderr encoding on Windows (lines 82-89)
- Sets `PYTHONIOENCODING=utf-8` for child processes

### External dependencies
- `argparse`, `json`, `re`, `sys`, `pathlib`, `os`, `ast`, `shutil`, `time`

---

## 8. `error_codes.py`

**Module purpose**: Structured error code system for better UX, automation, and support.

### Error Code Format
- Internal: `DS_NNN_SLUG` (Enum member name)
- Display: `[DS-NNN] Title: message`

### Classes

#### `ErrorCategory` (str, Enum)
- `VALIDATION`, `PARSING`, `CHUNKING`, `RESOURCE`, `CONFIG`, `SYSTEM`

#### `ErrorCode` (Enum)
Each member is `(code, title, category)`. Properties: `code`, `code_str`, `title`, `category`, `doc_url`.

| Code | Name | Category |
|------|------|----------|
| DS-001 | Invalid Context Path | validation |
| DS-002 | Invalid Session Hash | validation |
| DS-003 | Missing Query | validation |
| DS-004 | Invalid Chunk Size | validation |
| DS-005 | Overlap Exceeds Size | validation |
| DS-006 | Empty Context | validation |
| DS-101 | AST Parse Failed | parsing |
| DS-102 | JSON Decode Error | parsing |
| DS-103 | Encoding Error | parsing |
| DS-104 | Sub-agent Parse Failed | parsing |
| DS-105 | Checkpoint Corrupt | parsing |
| DS-201 | Chunk Too Large | chunking |
| DS-202 | No Chunks Created | chunking |
| DS-203 | Aggregation Conflict | chunking |
| DS-204 | Result Validation Failed | chunking |
| DS-205 | Batch Failed | chunking |
| DS-301 | File Not Found | resource |
| DS-302 | Permission Denied | resource |
| DS-303 | File Too Large | resource |
| DS-304 | Context Too Large | resource |
| DS-305 | Cache Directory Error | resource |
| DS-306 | Session Not Found | resource |
| DS-401 | Invalid Config File | config |
| DS-402 | Missing Required Setting | config |
| DS-403 | Invalid Model Setting | config |
| DS-404 | Escalation Budget Exceeded | config |
| DS-501 | Internal Error | system |
| DS-502 | State Corruption | system |
| DS-503 | Timeout Error | system |
| DS-504 | Rate Limit Error | system |
| DS-505 | Cancelled By User | system |

#### `ErrorContext` (Pydantic BaseModel)
- Fields: `file_path`, `chunk_id`, `session_id`, `expected`, `actual`, `extra`
- Uses `ConfigDict(extra="ignore")` for forward compatibility.

#### `DeepScanError` (Exception)
- **`__init__(self, code, message, context=None, cause=None)`**
- **`format_message(self) -> str`** -- Human-readable format.
- **`to_dict(self) -> dict`** -- JSON serialization.
- **`to_json(self) -> str`** -- JSON string.

### Functions
- **`get_remediation(code, context=None) -> str`** -- Template-based remediation hints with context substitution.
- **`get_exit_code(code) -> int`** -- Maps error to Unix exit code. DS-505 returns 130 (SIGINT convention).
- **`handle_error(error, verbose=False) -> int`** -- Prints formatted error with Rich (if available). Escapes user content for Rich markup injection prevention.

### Exit Code Mapping
| Category | Exit Code |
|----------|-----------|
| validation | 2 |
| parsing | 3 |
| chunking | 4 |
| resource | 5 |
| config | 6 |
| system | 1 |
| DS-505 (cancelled) | 130 |

### External dependencies
- `json`, `enum`, `pydantic`, `rich` (optional)

---

## 9. `grep_utils.py`

**Module purpose**: ReDoS-protected grep with process isolation.

### Functions

- **`is_safe_regex(pattern) -> bool`** -- Heuristic pre-filter using REDOS_PATTERNS from constants. Returns True if pattern appears safe.
- **`safe_grep(pattern, content, max_matches=20, window=100, timeout=GREP_TIMEOUT) -> list[dict]`** -- Two-layer protection:
  1. Heuristic pattern validation
  2. Process isolation with `multiprocessing.Process` and `terminate()`/`kill()` on timeout
  - Returns list of dicts: `{match, span, snippet}`
  - Raises: `ValueError` (dangerous pattern or content too large), `TimeoutError` (regex timeout), `RuntimeError` (invalid regex)
  - Content size limit: `MAX_GREP_CONTENT_SIZE` (5MB)
- **`_grep_worker(pattern, content, max_matches, window, result_queue)`** -- Worker process function.

### External dependencies
- `re`, `multiprocessing.Process`, `multiprocessing.Queue`, `queue.Empty`

---

## 10. `helpers.py`

**Module purpose**: Helper functions injected into REPL namespace. Created via `create_helpers(manager)` factory.

### Constants
- `AST_CHUNKER_AVAILABLE: bool` -- True if ast_chunker module can be imported.

### Functions

#### `create_helpers(manager: StateManager) -> dict[str, Any]`

Returns dict of 20 helpers. Each helper is a closure over the StateManager.

**Context Viewing**:
- **`peek(start=0, end=None) -> str`** -- View context substring. Returns lazy mode message if in lazy mode. Caps at MAX_CONTEXT_PREVIEW.
- **`peek_head(n=3000) -> str`** -- First n chars (capped at 50KB).
- **`peek_tail(n=3000) -> str`** -- Last n chars (capped at 50KB).
- **`context_length() -> int`** -- Lambda returning context length.

**Search**:
- **`grep(pattern, max_matches=20, window=100) -> list[dict]`** -- Uses safe_grep. Raises `LazyModeError` in lazy mode.
- **`grep_file(pattern, filepath, max_matches=20, window=100) -> list[dict]`** -- File-specific grep (works in lazy mode). Returns error dicts for various failure modes.

**Chunking**:
- **`chunk_indices(size=150000, overlap=0) -> list[tuple[int, int]]`** -- Validates: 50K <= size <= 300K, 0 <= overlap <= 50K, overlap < size. Raises `LazyModeError`.
- **`write_chunks(out_dir=None, size=150000, overlap=0, semantic=False) -> list[str]`** -- Write chunks to files. Semantic mode uses AST-based chunking. Chunk count safety: warns at 100, blocks at 500. Path traversal protection on out_dir.

**State Management**:
- **`add_buffer(text) -> None`**
- **`get_buffers() -> list[str]`**
- **`clear_buffers() -> None`**
- **`add_result(result: dict) -> None`** -- Validates as ChunkResult, updates progress.
- **`add_results_from_file(file_path: str) -> dict`** -- Batch import from JSON. File size limit: 10MB. Path traversal protection. Returns `{added, errors}`.
- **`set_phase(phase) -> None`**
- **`set_final_answer(answer) -> None`** -- Sets phase to "completed", progress to 100%.
- **`get_status() -> dict`** -- Returns session_id, phase, context_size, total_chunks, progress_percent.

**Lazy Mode**:
- **`is_lazy_mode() -> bool`**
- **`get_tree_view() -> str`** -- Returns cached tree view or generates on-demand.
- **`preview_dir(subpath, max_depth=2, max_files=30) -> str`** -- Preview subdirectory structure. Path traversal protection.
- **`load_file(filepath) -> str`** -- Load file content. 10MB limit. Binary file detection. Path traversal protection.

**Validation**: Runtime check that helper keys match `HELPER_NAMES` constant (emits `RuntimeWarning` on desync).

### External dependencies
- `warnings`, `pathlib`, `constants`, `grep_utils`, `models`, `walker`, `ast_chunker` (optional)

---

## 11. `incremental.py`

**Module purpose**: File hash tracking and delta detection for incremental re-analysis. Only changed files are re-processed.

### Constants
- **`DEFAULT_IGNORE_PATTERNS`** -- 16 patterns: `__pycache__`, `*.pyc`, `*.pyo`, `.git`, `.svn`, `.hg`, `node_modules`, `.env`, `.env.*`, `*.log`, `.DS_Store`, `Thumbs.db`, `*.key`, `*.pem`, `credentials*`, `*.secret`
- **`DEFAULT_HASH_ALGORITHM`** -- `HashAlgorithm.XXHASH3` if xxhash available, else `HashAlgorithm.SHA256`

### Enums

#### `HashAlgorithm`
- `SHA256` -- Cryptographic, slower (64-char hex)
- `XXHASH64` -- Fast, 64-bit hash (16-char hex)
- `XXHASH3` -- Fastest, 64-bit hash (16-char hex)

### Classes

#### `FileHash` (Pydantic BaseModel)
- Fields: `file_path`, `sha256`, `size`, `mtime`
- **`from_file(cls, file_path, base_path=None)`** -- Class method to create from actual file.

#### `FileDelta` (dataclass)
- Fields: `changed_files`, `added_files`, `deleted_files`
- Properties: `has_changes`, `total_changes`

#### `FileHashManifest` (Pydantic BaseModel)
- Fields: `file_hashes` (dict), `created_at` (UTC), `source_path`, `algorithm`
- **`from_directory(cls, directory, ignore_patterns=None, algorithm=None)`** -- Scan directory, skip symlinks, respect ignore patterns. Reads files in 64KB chunks.
- **`compare_with(self, previous) -> FileDelta`** -- Compute delta between manifests.
- **`save(self, path)`** / **`load(cls, path)`** -- JSON persistence.

#### `ChunkFileMapping` (Pydantic BaseModel)
- Fields: `mappings` (dict of file_path -> list[ChunkMappingInfo])
- **`add(self, chunk_id, file_path, start_offset, end_offset)`**
- **`get_chunks_for_file(self, file_path) -> list[str]`**
- **`get_affected_chunks(self, changed_files) -> set[str]`**

#### `ChunkMappingInfo` (TypedDict)
- Fields: `chunk_id`, `start_offset`, `end_offset`

#### `IncrementalAnalyzer`
- **`__init__(self, session_hash, cache_root=None)`** -- Validates session_hash.
- **`get_previous_manifest(self) -> FileHashManifest | None`** -- Load, returns None for missing/corrupted.
- **`save_manifest(self, manifest)`**
- **`get_affected_chunks(self, source_directory, ignore_patterns=None) -> FileDelta`**

### Functions
- **`is_xxhash_available() -> bool`**
- **`compute_file_hash(file_path, algorithm=DEFAULT_HASH_ALGORITHM) -> str`** -- Reads in 64KB chunks. Falls back to SHA-256 if xxhash unavailable.
- **`_should_ignore(path, patterns) -> bool`** -- Checks full path and path components against patterns.

### External dependencies
- `fnmatch`, `hashlib`, `logging`, `re`, `dataclasses`, `datetime`, `enum`, `pathlib`, `pydantic`, `xxhash` (optional)

---

## 12. `models.py`

**Module purpose**: Pydantic data models for all DeepScan state. All state serialized to JSON (no pickle).

### Security Constants
| Constant | Value | Description |
|----------|-------|-------------|
| `SESSION_HASH_PATTERN` | `^[a-zA-Z0-9_-]+$` | Path traversal prevention regex |
| `MAX_CHECKPOINT_WRITE_SIZE` | 20MB | New checkpoint size limit |
| `MAX_CHECKPOINT_READ_SIZE` | 100MB | Legacy checkpoint read limit |
| `MAX_CHECKPOINT_SIZE` | 20MB | Deprecated alias |

### Exceptions
- **`LazyModeError(RuntimeError)`** -- Raised when operation needs full context in lazy mode. Includes recovery instructions.

### Enums

#### `FailureType` (str, Enum)
| Value | Escalation |
|-------|-----------|
| `TIMEOUT` | No -- larger models are slower |
| `PARSE_ERROR` | No -- fix prompt instead |
| `RATE_LIMIT` | No -- retry same model |
| `QUALITY_LOW` | Yes -- model capability issue |
| `COMPLEXITY` | Yes -- code too complex |
| `UNKNOWN` | No |

#### `ScanMode` (str, Enum)
- `FULL` -- Load everything (default)
- `LAZY` -- Structure only, lazy loading
- `TARGETED` -- Specific files/patterns

### Data Models

#### `ChunkInfo` (Pydantic BaseModel)
- Fields: `chunk_id`, `file_path`, `start_offset`, `end_offset`, `size`, `status` (default "pending"), `processed_at`

#### `Finding` (Pydantic BaseModel)
- Fields: `point`, `evidence`, `confidence` (default "medium"), `location` (optional dict), `verification_required` (default False)

#### `ChunkResult` (Pydantic BaseModel)
- Fields: `chunk_id`, `status`, `findings`, `missing_info`, `suggested_queries`, `partial_answer`, `error`, `processed_at`

#### `ContextMetadata` (Pydantic BaseModel)
- Fields: `path`, `loaded_at`, `total_size`, `encoding` (default "utf-8"), `is_directory`, `file_count`

#### `DeepScanConfig` (Pydantic BaseModel)
| Field | Default | Description |
|-------|---------|-------------|
| `chunk_size` | 150,000 | Characters per chunk |
| `chunk_overlap` | 0 | Overlap between chunks |
| `max_parallel_agents` | 5 | Concurrent sub-agents |
| `max_retries` | 3 | Retry count |
| `timeout_seconds` | 300 | Operation timeout |
| `adaptive_chunking` | False | Auto-detect content type |
| `detected_content_type` | None | e.g. "code:.py" |
| `enable_escalation` | True | Auto-upgrade on failures |
| `max_escalation_ratio` | 0.15 | Max 15% chunk escalation |
| `max_sonnet_cost_usd` | 5.0 | Cost cap |
| `incremental_enabled` | False | Delta analysis |
| `previous_session` | None | Session for comparison |
| `changed_file_count` | 0 | Changed/added files |
| `deleted_file_count` | 0 | Deleted files |
| `scan_mode` | ScanMode.FULL | Traversal strategy |
| `lazy_depth` | 3 | Lazy traversal depth |
| `lazy_file_limit` | 50 | Lazy mode file cap |
| `target_paths` | [] | Targeted mode paths |
| `agent_type` | "general" | Analysis specialization |

#### `DeepScanState` (Pydantic BaseModel)
- Fields: `version` ("2.0.0"), `session_id` (auto-generated), `created_at`, `updated_at`, `config`, `context_meta`, `query`, `chunks`, `results`, `buffers`, `phase` (default "initialized"), `progress_percent`, `final_answer`
- `model_post_init` updates `updated_at` on creation.
- Phase values: "initialized", "scouting", "chunking", "mapping", "reducing", "completed"

### External dependencies
- `re`, `secrets`, `time`, `datetime`, `enum`, `pydantic`

---

## 13. `progress.py`

**Module purpose**: Progress streaming, escalation management, and session validation.

### Functions

- **`should_escalate(failure_type, attempt) -> bool`** -- Escalate only for QUALITY_LOW/COMPLEXITY after attempt >= 2.
- **`classify_failure(error_message, response_length=0) -> FailureType`** -- Pattern matching on error messages. response_length=0 -> UNKNOWN, 1-49 -> QUALITY_LOW.
- **`validate_session_hash(session_hash) -> bool`** -- Regex + `..` / `/` / `\` check.

### Classes

#### `EscalationBudget`
- **`__init__(self, max_escalation_ratio=0.15, max_sonnet_cost_usd=5.0)`**
- **`can_escalate(self) -> bool`** -- Checks ratio and cost limits.
- **`record_escalation(self, estimated_cost=0.01)`**
- **`set_total_chunks(self, total)`**

#### `ProgressWriter`
- **`__init__(self, session_dir, max_size=10MB)`** -- JSONL file writer.
- Context manager (`__enter__`/`__exit__`).
- **`emit(self, event_type, **data)`** -- Write event with timestamp. Flush immediately for `tail -f`.
- **`_rotate_if_needed(self)`** -- Rotates file when exceeding max_size (renames to .jsonl.1).
- Event methods: `emit_batch_start`, `emit_batch_end`, `emit_chunk_complete`, `emit_finding`, `emit_escalation`.

### External dependencies
- `json`, `datetime`, `pathlib`

---

## 14. `repl_executor.py`

**Module purpose**: Session-scoped subprocess for safe REPL execution with timeout (P7-001).

### Classes

#### `SafeREPLExecutor`
- **`__init__(self, timeout=DEFAULT_EXEC_TIMEOUT)`** -- Starts worker subprocess.
- **`_start_worker(self)`** -- Creates daemon `Process` with worker loop.
- **`_worker_loop` (static)** -- Worker process main loop. Applies resource limits on Unix:
  - Memory: 256MB (soft), 512MB (hard)
  - CPU time: 60s (soft), 120s (hard)
  - Max file size: 10MB
  - Falls back silently on Windows (no `resource` module)
- Uses `eval()` first, falls back to `exec()` on SyntaxError.
- **`execute(self, code) -> Any`** -- Send code, get result with timeout. On timeout: terminate worker, restart, raise `TimeoutError`.
- **`_terminate_worker(self)`** -- `terminate()` then `kill()` with timeouts.
- **`shutdown(self)`** -- Graceful shutdown with shutdown signal.
- **`__del__`** -- Cleanup on GC.

### Functions

- **`get_repl_executor(timeout=5, reset=False) -> SafeREPLExecutor`** -- Thread-safe singleton.
- **`reset_global_state()`** -- For test isolation. Resets REPL executor and cancellation manager.
- **`_execute_with_thread_timeout(code, namespace, timeout=5) -> tuple[str, Any]`** -- Thread-based timeout for helper execution. **ZOMBIE_THREAD_WARNING**: Python threads cannot be forcibly killed. If code hangs, thread continues in background.

### Security
- Worker process has resource limits (Unix only)
- SAFE_BUILTINS namespace
- Subprocess isolation with terminate/kill capability

### External dependencies
- `logging`, `threading`, `multiprocessing.Process`, `multiprocessing.Queue`, `queue.Empty`

---

## 15. `state_manager.py`

**Module purpose**: State persistence with Pydantic + JSON. Handles session lifecycle, file traversal, and context loading.

### Module-level Functions

- **`_parse_deepscanignore(root_path) -> tuple[set[str], list[str]]`** -- Parse `.deepscanignore` file (gitignore-like format). Returns (dir_patterns, glob_patterns).
- **`_should_skip_path(path, root_path=None, custom_dirs=None, custom_globs=None) -> bool`** -- Combines DEFAULT_PRUNE_DIRS + custom patterns.
- **`_calculate_entry_size(content, rel_path) -> tuple[str, str, int]`** -- DRY helper for file entry overhead (header + footer).

### Classes

#### `StateManager`
**Class Constants**:
- `DEFAULT_CACHE_ROOT = Path.home() / ".claude" / "cache" / "deepscan"`
- `STATE_FILE = "state.json"`
- `CURRENT_SESSION_FILE = Path.home() / ".claude" / "cache" / "deepscan" / ".current_session"`

**Class Methods**:
- **`list_sessions(cls) -> list[dict]`** -- Lists all sessions sorted by mtime (newest first).
- **`get_current_session_hash(cls) -> str | None`** -- Read `.current_session` marker file.
- **`set_current_session_hash(cls, session_hash)`** -- Atomic write via tempfile + rename. Handles double-close on error.
- **`gc_clean_old_sessions(cls, max_age_days=7, max_total_size_gb=1.0) -> dict`** -- TTL and LRU eviction. Returns `{deleted, freed_bytes}`.

**Instance Methods**:
- **`__init__(self, session_hash=None)`** -- Generates hash if not provided. Thread lock for concurrency.
- **`lazy_tree_view` (property)** -- Public getter for `_lazy_tree_view`.
- **`ensure_dirs(self)`** -- Creates chunks/, results/, logs/ subdirectories.
- **`load(self) -> DeepScanState`** -- Thread-safe JSON load.
- **`_safe_write(self, path, content)`** -- Path traversal protection via `resolve().relative_to()`. Only allows writes to DEFAULT_CACHE_ROOT.
- **`save(self)`** -- Atomic write (temp + rename) with path validation. Thread-safe.
- **`init(self, context_path, query=None, adaptive=False, incremental=False, previous_session=None, lazy=False, target=None, depth=None, agent_type="general") -> DeepScanState`** -- Main initialization. File size limits:
  - Single file: 10MB max
  - Total context: 50MB max
  - Supports three modes: full, lazy, targeted
  - Parses .deepscanignore
  - Path traversal protection in targeted mode
  - Incremental analysis with FileHashManifest
- **`reset(self)`** -- Delete session directory.
- **`get_context(self) -> str`** -- Lazy-loads context. In lazy mode returns "". Path traversal protection and symlink blocking.

### Security Features
- Path traversal prevention in `_safe_write()` (lines 381-398)
- Symlink blocking in all traversal modes
- Session hash validation
- .deepscanignore support
- Atomic writes for session marker
- Thread-safe operations with `_lock`

### External dependencies
- `json`, `secrets`, `sys`, `threading`, `time`, `datetime`, `pathlib`, `shutil`, `tempfile`, `os`, `fnmatch`

---

## 16. `subagent_prompt.py`

**Module purpose**: Secure prompt generation for sub-agent MAP phase processing. Uses XML boundary structure to prevent indirect prompt injection.

### Constants
- **`SUPPORTED_AGENT_TYPES`** -- `["general", "security", "architecture", "performance"]`
- **`AgentType`** -- `Literal["general", "security", "architecture", "performance"]`
- **`AGENT_TYPE_INSTRUCTIONS`** -- Dict of specialized system instructions per agent type. Each includes VERIFICATION RULES to reduce false positives.
- **`SUBAGENT_PROMPT_TEMPLATE`** -- XML-structured template with sections: SYSTEM_INSTRUCTIONS, CHUNK_METADATA, DATA_CONTEXT, USER_QUERY, OUTPUT_FORMAT, TERMINATION_MARKERS.

### Functions

- **`get_supported_agent_types() -> list[str]`** -- Returns copy of supported types.
- **`_sanitize_xml_content(content) -> str`** -- Prevents XML boundary escape attacks:
  1. Escapes XML comments (`<!-- -->`)
  2. Escapes CDATA end sequences (`]]>`)
  3. Case-insensitive regex for boundary tag names (DATA_CONTEXT, USER_QUERY, SYSTEM_INSTRUCTIONS, CHUNK_METADATA, OUTPUT_FORMAT)
  4. Unicode lookalike detection and replacement (14 unicode characters that visually resemble `<`, `>`, `/`)
- **`generate_subagent_prompt(chunk, chunk_content, query, chunk_number, total_chunks, agent_type="general") -> str`** -- Validates inputs, sanitizes content, formats template. Raises `ValueError` for invalid inputs.
- **`parse_subagent_response(response, chunk_id) -> dict`** -- Extracts JSON from markdown code blocks or raw response. Returns ChunkResult-compatible dict.
- **`create_sequential_prompt(chunk, chunk_content, query) -> str`** -- Simplified prompt for sequential fallback. Also uses XML boundaries.

### Output Format (expected from sub-agents)
```json
{
  "chunk_id": "...",
  "status": "completed",
  "findings": [{"point": "...", "evidence": "...", "confidence": "high|medium|low", "location": {"context": "..."}}],
  "missing_info": ["..."],
  "partial_answer": "..."
}
```

### Termination Markers
- `FINAL({"result": "answer"})` -- Definitive answer
- `FINAL_VAR(variable_name)` -- REPL variable reference
- `NEEDS_MORE("reason")` -- Need more chunks
- `UNABLE("reason")` -- Cannot complete

### External dependencies
- `re` (for sanitization)

---

## 17. `walker.py`

**Module purpose**: Generator-based directory tree walker using `os.scandir`. Memory-efficient lazy traversal.

### Constants
| Constant | Value | Description |
|----------|-------|-------------|
| `DEFAULT_TREE_VIEW_LIMIT` | 10,000 | Safety cap (also in constants.py) |
| `TREE_BRANCH` | `"--- "` | Non-last child connector |
| `TREE_LAST` | `"--- "` | Last child connector |
| `TREE_VERTICAL` | `"|   "` | Vertical continuation |
| `TREE_EMPTY` | `"    "` | Empty space |
| `DEFAULT_PRUNE_DIRS` | frozenset of 16 dirs | Common dirs to skip |

### DEFAULT_PRUNE_DIRS
`node_modules`, `.git`, `.svn`, `.hg`, `__pycache__`, `.venv`, `venv`, `.env`, `env`, `.tox`, `.pytest_cache`, `.mypy_cache`, `.ruff_cache`, `dist`, `build`, `.next`, `.nuxt`, `target`, `vendor`

### Classes

#### `FileEntry` (frozen dataclass with slots)
- Fields: `path` (Path), `name` (str), `is_dir` (bool), `size` (int), `mtime` (datetime UTC), `depth` (int)
- Compact `__repr__` for debugging.

### Functions

- **`format_size(size_bytes) -> str`** -- Human-readable: B, KB, MB, GB.
- **`tree_explore(start_path, max_depth=None, max_files=None, should_prune=None) -> Iterator[FileEntry]`** -- Main generator. Uses `os.scandir` for Windows performance. Features:
  - Validates max_depth >= 0, max_files >= 0
  - Filters BEFORE sorting for performance
  - Sorts: directories first, then alphabetical (case-insensitive)
  - `follow_symlinks=False` throughout
  - PermissionError/OSError logged and skipped
  - max_files counts BOTH files AND directories
- **`generate_tree_view(start_path, max_depth=None, max_files=None, should_prune=None, show_size=True, show_hidden=False) -> str`** -- ASCII tree view. Safety cap at DEFAULT_TREE_VIEW_LIMIT. Combines hidden file filter with user prune function. Adds summary with file count, total size, depth, truncation warning.
- **`default_should_prune(path) -> bool`** -- Returns `path.name in DEFAULT_PRUNE_DIRS`.

### Security
- Symlinks NOT followed (`follow_symlinks=False`)
- PermissionError handled gracefully
- Max depth enforcement
- Max files safety cap

### External dependencies
- `logging`, `os`, `collections.abc`, `dataclasses`, `datetime`, `pathlib`

---

## Cross-Cutting Concerns Summary

### Security Mechanisms by Module
| Module | Mechanism |
|--------|-----------|
| `deepscan_engine.py` | 3-layer sandbox (forbidden patterns, AST whitelist, attribute blocking) |
| `repl_executor.py` | Subprocess isolation, resource limits, timeout |
| `grep_utils.py` | ReDoS protection (heuristic + process isolation) |
| `state_manager.py` | Path traversal protection (`_safe_write`), symlink blocking |
| `ast_chunker.py` | Project-root enforcement, max recursion depth |
| `checkpoint.py` | Session hash validation, file size limits, path traversal |
| `subagent_prompt.py` | XML sanitization, Unicode lookalike detection, prompt injection prevention |
| `cancellation.py` | Re-entrancy safe signal handling, deadlock prevention |
| `walker.py` | No symlink following, PermissionError handling |
| `constants.py` | SAFE_BUILTINS allowlist, REDOS_PATTERNS |
| `helpers.py` | Path traversal in write_chunks/load_file/preview_dir, file size limits, binary detection |
| `incremental.py` | Session hash validation, symlink skipping, ignore patterns |

### Configuration Options (User-Facing)
| Option | Location | Default | Description |
|--------|----------|---------|-------------|
| `--adaptive` | CLI init | False | Auto chunk sizing |
| `--incremental` | CLI init | False | Delta analysis |
| `--lazy` | CLI init | False | Structure-only mode |
| `--target PATH` | CLI init | None | Targeted paths |
| `--depth N` | CLI init | 3 | Lazy traversal depth |
| `--agent-type TYPE` | CLI init | "general" | Analysis specialization |
| `--force` | CLI init | False | Overwrite session |
| `--watch` | CLI progress | False | Real-time monitoring |
| `--instructions` | CLI map | False | Generate Task tool prompts |
| `--escalate` | CLI map | False | Retry with sonnet model |
| `--output FILE` | CLI map | None | Save instructions to file |
| `--batch N` | CLI map | None | Show specific batch |
| `--limit N` | CLI map | 5 | Max chunks in output |
| `--timeout N` | CLI exec | auto | Execution timeout |
| `--older-than N` | CLI clean | 7 | Days before cleanup |
| `.deepscanignore` | Project root | - | Custom ignore patterns |
| `similarity_threshold` | ResultAggregator | 0.7 | Deduplication threshold |
| `graceful_timeout` | CancellationManager | 10.0 | Shutdown timeout |
| `max_escalation_ratio` | DeepScanConfig | 0.15 | Escalation budget ratio |
| `max_sonnet_cost_usd` | DeepScanConfig | 5.0 | Sonnet cost cap |
| `max_parallel_agents` | DeepScanConfig | 5 | Concurrent sub-agents |
| `max_retries` | DeepScanConfig | 3 | Retry count |
| `timeout_seconds` | DeepScanConfig | 300 | Operation timeout |

### Error Codes (User-Facing)
All 26 error codes documented in `error_codes.py` with remediation hints.

### File Size Limits
| Limit | Value | Location |
|-------|-------|----------|
| Single file | 10MB | state_manager.py:445 |
| Total context | 50MB | state_manager.py:446 |
| Checkpoint write | 20MB | models.py:51 |
| Checkpoint read | 100MB | models.py:56 |
| Grep content | 5MB | constants.py:70 |
| CLI output | 100KB | constants.py:68 |
| Context preview | 50KB | constants.py:69 |
| Output per operation | 500KB | constants.py:67 |
| REPL code input | 100KB | deepscan_engine.py:339 |
| Import file | 10MB | helpers.py:408 |
| Load file | 10MB | helpers.py:621 |
| Worker memory (soft) | 256MB | repl_executor.py:87 |
| Worker memory (hard) | 512MB | repl_executor.py:87 |
| Worker CPU time (soft) | 60s | repl_executor.py:89 |
| Worker CPU time (hard) | 120s | repl_executor.py:89 |
| Worker file size | 10MB | repl_executor.py:91 |
| Progress file | 10MB | constants.py:225 |

### Undocumented / Potentially Surprising Features
1. **CLI shortcuts** (`?`, `!`, `+`, `x`, auto-path-detection) in deepscan_engine.py
2. **`.deepscanignore`** file support in state_manager.py
3. **Unicode lookalike detection** in subagent_prompt.py sanitization
4. **Zombie thread warning** for helper execution path (repl_executor.py)
5. **Asymmetric checkpoint limits** (read=100MB, write=20MB) for backward compatibility
6. **Ghost findings cleanup** (P7-003) in aggregator.py
7. **NEEDS_VERIFICATION prefix** handling in aggregator.py
8. **Placeholder vs pending result distinction** in deepscan_engine.py MAP phase
9. **HMAC signature was explicitly decided against** for state files (state_manager.py)
10. **Tree view counts both files AND directories** in max_files (walker.py)
11. **`context_length` helper** is a lambda, not a named function
12. **`re` module was explicitly removed** from REPL namespace (CVE-2026-002 fix comment in helpers.py)
13. **Windows UTF-8 reconfigure** at module import time in deepscan_engine.py
14. **Session ID format**: `deepscan_{timestamp}_{16_hex_chars}`
15. **Progress JSONL rotation** to `.jsonl.1` on size limit
16. **Adaptive chunking** auto-detects content type from file extensions
17. **Model escalation** only triggers for QUALITY_LOW and COMPLEXITY failures after attempt >= 2
18. **Contradiction detection** uses bidirectional negation heuristic with 0.4 threshold
19. **Semantic chunking** supports 5 languages (Python, JavaScript, TypeScript, Java, Go)
20. **Chunk count safety**: warns at 100, hard-blocks at 500
